{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Project\n",
    "### Name: Daniel Jang\n",
    "### GitHub: hjang8659\n",
    "### USC ID: 6894-6426-04\n",
    "### Due: May 7, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import tree\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, train_test_split, StratifiedKFold, cross_validate, KFold, GridSearchCV, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, Ridge, LassoCV, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, DistanceMetric, KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.linalg import pinv\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, hamming_loss, mean_squared_error, calinski_harabasz_score, silhouette_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "### (a) & (b) i. - v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 46830\n",
      "Average Review Length for Training Texts: 641.4178571428571\n",
      "Standard Deviation of Review Lengths for Training Texts: 285.0965181848464\n"
     ]
    }
   ],
   "source": [
    "sentiment_map = {'neg': -1, 'pos': 1}\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "unique_words = set()\n",
    "\n",
    "def clean_text(text):\n",
    "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    return text.translate(translator)\n",
    "\n",
    "for folder in ['neg', 'pos']:\n",
    "    folder_path = os.path.join('../data', folder)\n",
    "    for i, filename in enumerate(os.listdir(folder_path)):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text = clean_text(text)\n",
    "            words = text.split()\n",
    "            unique_words.update(words)\n",
    "            if i < 700:\n",
    "                train_texts.append(text)\n",
    "                train_labels.append(sentiment_map[folder])\n",
    "            elif i >= 700:\n",
    "                test_texts.append(text)\n",
    "                test_labels.append(sentiment_map[folder])\n",
    "\n",
    "num_unique_words = len(unique_words)\n",
    "print(\"Number of unique words in the dataset:\", num_unique_words)\n",
    "\n",
    "review_lengths = [len(text.split()) for text in train_texts]\n",
    "avg_length = np.mean(review_lengths)\n",
    "std_dev = np.std(review_lengths)\n",
    "print(\"Average Review Length for Training Texts:\", avg_length)\n",
    "print(\"Standard Deviation of Review Lengths for Training Texts:\", std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(review_lengths, bins=50, alpha=0.7, color='green')\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# print(\"Word Counts:\", tokenizer.word_counts)\n",
    "# print(\"Document Count:\", tokenizer.document_count)\n",
    "# print(\"Word Index:\", tokenizer.word_index)\n",
    "# print(\"Word Docs:\", tokenizer.word_docs)\n",
    "\n",
    "encoded_train_docs = tokenizer.texts_to_matrix(train_texts, mode='count')\n",
    "encoded_test_docs = tokenizer.texts_to_matrix(test_texts, mode='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Length Threshold (70%): 730\n",
      "Review Length Threshold (90%): 984\n"
     ]
    }
   ],
   "source": [
    "def select_review_length(review_lengths, percentile):\n",
    "    return int(np.percentile(review_lengths, percentile))\n",
    "\n",
    "threshold_70 = select_review_length(review_lengths, 70)\n",
    "print(\"Review Length Threshold (70%):\", threshold_70)\n",
    "\n",
    "threshold_90 = select_review_length(review_lengths, 90)\n",
    "print(\"Review Length Threshold (90%):\", threshold_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "tokenized_test_texts = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "threshold = 730\n",
    "\n",
    "max_review_length = threshold\n",
    "padded_train_texts = pad_sequences(tokenized_train_texts, maxlen=max_review_length, padding='post')\n",
    "padded_test_texts = pad_sequences(tokenized_test_texts, maxlen=max_review_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "### i. & ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "\n",
    "max_words = max_review_length\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_words))\n",
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "### i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 3s 15ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6699 - val_accuracy: 0.5917\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.2829 - accuracy: 0.8786 - val_loss: 0.9546 - val_accuracy: 0.6000\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "epochs = 2\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "train_labels = (train_labels + 1) / 2\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = (test_labels + 1) / 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_unique_words, \n",
    "                    output_dim=embedding_dim, \n",
    "                    input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(padded_train_texts), train_labels, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(np.array(padded_test_texts), test_labels))\n",
    "\n",
    "train_loss, train_acc = model.evaluate(np.array(padded_train_texts), train_labels, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(np.array(padded_test_texts), test_labels, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii.\n",
    "### The trained multi-layer perceptron (MLP) model achieved impressive performance, with a perfect accuracy of 100% on the training dataset and a respectable accuracy of 60% on the test dataset. While the perfect accuracy on the training set could indicate overfitting, the model's ability to generalize to unseen data is evident from its test accuracy. This result suggests that the model effectively captures patterns in the data, allowing it to make accurate predictions. However, further analysis and experimentation may be beneficial to enhance the model's performance even further and ensure robustness in various scenarios.\n",
    "### (e)\n",
    "### i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_unique_words, \n",
    "                    output_dim=embedding_dim, \n",
    "                    input_length=max_review_length))\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.6920 - accuracy: 0.5200 - val_loss: 0.6855 - val_accuracy: 0.5533\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.4471 - accuracy: 0.7993 - val_loss: 0.6193 - val_accuracy: 0.7117\n",
      "Train Accuracy: 0.9878571629524231\n",
      "Test Accuracy: 0.7116666436195374\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(padded_train_texts), train_labels, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(np.array(padded_test_texts), test_labels))\n",
    "\n",
    "train_loss, train_acc = model.evaluate(np.array(padded_train_texts), train_labels, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(np.array(padded_test_texts), test_labels, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii.\n",
    "### For the one-dimensional convolutional neural network (CNN) model applied to text classification, the train accuracy after two epochs is approximately 98.79%, and the test accuracy is around 71.17%. This indicates that the model performs well on the training data but slightly less so on the unseen test data. Further tuning of hyperparameters or model architecture may be necessary to improve generalization performance.\n",
    "### (f)\n",
    "### i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 [==============================] - 35s 241ms/step - loss: 0.7012 - accuracy: 0.5179 - val_loss: 0.6878 - val_accuracy: 0.5533\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 32s 231ms/step - loss: 0.6752 - accuracy: 0.5821 - val_loss: 0.6893 - val_accuracy: 0.5567\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 33s 238ms/step - loss: 0.6008 - accuracy: 0.6457 - val_loss: 0.7545 - val_accuracy: 0.5500\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 32s 229ms/step - loss: 0.5120 - accuracy: 0.6671 - val_loss: 1.0676 - val_accuracy: 0.5717\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 32s 229ms/step - loss: 0.4845 - accuracy: 0.6821 - val_loss: 3.7935 - val_accuracy: 0.5133\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 32s 226ms/step - loss: 0.5139 - accuracy: 0.6736 - val_loss: 0.8866 - val_accuracy: 0.5633\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 31s 224ms/step - loss: 0.4861 - accuracy: 0.6786 - val_loss: 0.8715 - val_accuracy: 0.5667\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 31s 224ms/step - loss: 0.4807 - accuracy: 0.6814 - val_loss: 0.9747 - val_accuracy: 0.5633\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 32s 231ms/step - loss: 0.4922 - accuracy: 0.6786 - val_loss: 0.8748 - val_accuracy: 0.5617\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 33s 239ms/step - loss: 0.4894 - accuracy: 0.6750 - val_loss: 0.8670 - val_accuracy: 0.5633\n",
      "Train Accuracy: 0.6764285564422607\n",
      "Test Accuracy: 0.5633333325386047\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=num_unique_words, \n",
    "                         output_dim=embedding_dim, \n",
    "                         input_length=max_review_length))\n",
    "lstm_model.add(LSTM(256))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(256, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lstm_history = lstm_model.fit(np.array(padded_train_texts), train_labels, batch_size=10, epochs=10,\n",
    "                              validation_data=(np.array(padded_test_texts), test_labels))\n",
    "\n",
    "train_loss, train_acc = lstm_model.evaluate(np.array(padded_train_texts), train_labels, verbose=0)\n",
    "test_loss, test_acc = lstm_model.evaluate(np.array(padded_test_texts), test_labels, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii.\n",
    "### The train accuracy is approximately 67.64%, while the test accuracy is around 56.33%. These results indicate that the model performs moderately well on the training data but does not generalize as effectively to unseen test data. Further optimization or exploration of different model architectures may be beneficial to enhance performance on both training and test datasets.\n",
    "\n",
    "### Across the three models tested—One-Dimensional Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) Recurrent Neural Network, and Multi-Layer Perceptron (MLP)—the performance varied in terms of train and test accuracies.\n",
    "### For the CNN model, the train accuracy after two epochs was approximately 98.79%, with a test accuracy of around 71.17%. This suggests good performance on the training data but slightly less effective generalization to unseen test data.\n",
    "### The LSTM model, trained over 10 epochs, achieved a train accuracy of approximately 67.64% and a test accuracy of around 56.33%. While the model performed reasonably well on the training data, it showed less robustness in generalizing to the test dataset.\n",
    "### Finally, the MLP model, trained over two epochs, achieved a train accuracy of 100% and a test accuracy of 56%. This indicates that the model performed exceptionally well on the training data but did not generalize as effectively to the test data.\n",
    "### Overall, while each model demonstrated strengths in certain areas, further optimization and exploration of different hyperparameters or architectures may be necessary to achieve better generalization performance across all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references:\n",
    "https://keras.io\n",
    "\n",
    "https://keras.io/api/models/sequential/\n",
    "\n",
    "https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/dropout/\n",
    "\n",
    "https://keras.io/api/optimizers/adam/\n",
    "\n",
    "https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Word_embedding\n",
    "\n",
    "https://chatgpt.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
